{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kickstart the wrangling process, all three steps were sequentially followed – gathering, assessing, and cleaning. \n",
    "\n",
    "All three sources of data were gathered in varying ways. The necessary libraries were first imported to ease the wrangling process. The first dataset was downloaded in a CSV format from the Udacity: Data Analyst Nanodegree module and was thereafter read into a Pandas data-frame called “archive”.  The second dataset was a TSV file located at a URL provided by Udacity. This was downloaded using the requests library. The final dataset was generated programmatically using the Twitter API format provided by Udacity. I used this due to my inability to get the Twitter Developer access.\n",
    "\n",
    "In the Assessment Phase, both types of assessment were performed – Visual and Programmatic. Observed issues were classified into two – Quality Issues and Tidiness issues. Codes were subsequently written to address these. Columns that had several null values were dropped. Only dog names starting with uppercase were recognized as valid and retained. The source column combined urls and texts. This was further split to tidy up the data. \n",
    "\n",
    "Copies of all the datasets were created before starting the Cleaning Process. This phase had three layers, involving multiple iterations. The sequence followed was marked by defining the cleaning action, programmatically writing lines of code to clean, the testing the cleaned data. Redundant columns were dropped in order to better organize the datasets. Dog names were restricted to characters beginning with uppercases, as names beginning with lowercases seemed random and invalid. Data types were adjusted (e.g. Timestamp data type was converted to date-type). Each of the three datasets were visually inspected and aberrations cleaned before merging all into one data-frame. I found it significantly easier to use the tidier, merged dataframe to generate insights and visualizations.\n",
    "\n",
    "To conclude the whole data wrangling process, the gathered, assessed and cleaned master dataset was stored into its own CSV file format.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
